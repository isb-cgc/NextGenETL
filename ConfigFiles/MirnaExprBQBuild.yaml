
version: 1
files_and_buckets_and_tables:
  # Run all BQ jobs in Batch mode? Slower but uses less of quotas:
  BQ_AS_BATCH: False

  # GDC downloader fills out a directory tree. Here is the root:
  LOCAL_FILES_DIR: directory_root_for_files

  # Maximum number of files to download. Useful for testing before doing full download.
  # If not set (comment out), will use no LIMIT for BQ:
  MAX_FILES: 20

  # We glue all the files into one big one before uploading into the staging bucket, here:
  ONE_BIG_TSV: raw_data_file_relative_path_and_file.tsv

  # Where do we stash the manifest after we get it. Even a BQ-generated manifest (which also
  # keeps a BQ table version) gets written here.
  MANIFEST_FILE: manifest_file_relative_path_and_file.tsv

  # Where do we put the manifest as a bq table:
  BQ_MANIFEST_TABLE: Manifest_BQ_Table_Name

  # Where do we put the pull list as a bq table:
  BQ_PULL_LIST_TABLE: Pull_List_BQ_Table_Name

  # Where do we stash the manifest in the bucket after we get it:
  BUCKET_MANIFEST_TSV: bucket_path_for_manifest.tsv # DO NOT HAVE A LEADING /

  # Where do we stash the pull_list in the bucket after we get it:
  BUCKET_PULL_LIST: bucket_path_for_pull_list.txt # DO NOT HAVE A LEADING /

  # Where do we locally stash the pull_list after we get it, or where is it if already provided:
  LOCAL_PULL_LIST: pull_list_file_full_path_and_file.tsv

  # Download creates a directory tree, which we need to traverse on upload. Put it here:
  FILE_TRAVERSAL_LIST: traversal_list_file_full_path_and_file.tsv

  # What bucket is going to get the text file heading to BQ?
  WORKING_BUCKET: bq_staging_bucket_name

  # What will be the file in the bucket (TSV):
  BUCKET_SKEL_TSV: bucket_path_for_raw_bq_table.tsv # DO NOT HAVE A LEADING /

  # What project are we in:
  WORKING_PROJECT: working_project_id

  # Where is the BQ table dataset:
  TARGET_DATASET: bq_dataset_for_table

  # Where is the skeleton table:
  SKELETON_TABLE: Skeleton_Table_Name

  # Where is the table that we will use to replace indexD calls:
  INDEXD_BQ_TABLE: project_name.dataset_name.manifest_table_name

  # Where is the table that maps files to aliquot:
  FILE_TABLE:  project_name.dataset_name.file_to_aliquot_table_name

  # Where is the table that maps aliquot ID to all the other stuff (e.g. case barcode):
  ALIQUOT_TABLE: project_name.dataset_name.aliquot_to_other_stuff_table_name

  # Some intermediate steps:
  BARCODE_STEP_1_TABLE: Intermediate_Table_One
  BARCODE_STEP_2_TABLE: Intermediate_Table_Two

  # Final table name:
  FINAL_TARGET_TABLE: TCGA_HG38_miRNA_Expression_Oct_2019_Final

  # Optional prefix to glue onto tumor type:
  PROGRAM_PREFIX: ForExampleTCGA-

  # Saved schema dict (all fields):
  HOLD_SCHEMA_DICT: schema_dict_file_relative_path_and_file.json

  # Saved schema list (typed tuples from schema analysis only)
  HOLD_SCHEMA_LIST: saved_schema_list_file_relative_path_and_file.json

  # Schema descriptions for all columns:
  AUGMENTED_SCHEMA_FILE: augmented_schema_list_file_relative_path_and_file.json

  # Number of rows to skip while sampling big TSV to generate schema:
  SCHEMA_SAMPLE_SKIPS: 100

  # Table description:
  TABLE_DESCRIPTION: Table holding miRNA expression file content from GDC.

# Extra fields being added to the ones present in the input files. WARNING! The semantics of these fields
# are NOT modified here...these four fields match the order generated in the file_info() function; this
# just provides the heading names.

extra_fields:
  - fileUUID

# This is the format provided if you choose to use the bq table to build a manifest.
# This needs the table column names

bq_filters:
   - program_name: TCGA
   - access: open
   - data_format: TXT
   - data_type: miRNA Expression Quantification
   - data_category: Transcriptome Profiling

steps:

  # Empty out the LOCAL_FILES_DIR before loading files (Recommended!):
  - clear_target_directory

  # This step builds MANIFEST_FILE from filters.
  - build_manifest_from_filters

  # Build the pull list (gs:// URLs to pull). Skip if one is already provided by LOCAL_PULL_LIST.
  - build_pull_list

  # Run the downloader on the manifest into LOCAL_FILES_DIR. This can take a while:
  - download_from_gdc

  # Build a file list from traversing LOCAL_FILES_DIR, write to FILE_TRAVERSAL_LIST. Need this for
  # the concatenation of all files into the ONE_BIG_TSV:
  - build_traversal_list

  # Build the ONE_BIG_TSV file from all the little files:
  - concat_all_files

  # Figure out the column types by build a schema from the ONE_BIG_TSV file
  - build_the_schema

  # Upload ONE_BIG_TSV to WORKING_BUCKET/BUCKET_SKEL_TSV:
  - upload_to_bucket

  # Load BQ table TARGET_DATASET.SKELETON_TABLE:
  - create_bq_from_tsv

  # Fold in fields from FILE_TABLE and ALIQUOT_TABLE to pull in barcodes:
  - collect_barcodes

  # Create the final table FINAL_TARGET_TABLE
  - create_final_table

  # Install final table schema descriptions:
  - update_final_schema

  # Add a table description:
  - add_table_description

  # Dump working tables, just keeping final results:
  - dump_working_tables
