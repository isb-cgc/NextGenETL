
version: 2

# Parameters for the workflow
files_and_buckets_and_tables:
  ## !Update these parameters per run of the workflow! ##

  # Run all BQ jobs in Batch mode? Slower but uses less of quotas:
  BQ_AS_BATCH: False

  # Which GDC Program are we grabbing data from?
  PROGRAM: GDC_PROGRAM

  # What year/month was the pipeline started in?
  DATE: YYYY-MM

  # Release:
  RELEASE: XX

  ## Previous release (may not be the number directly before the release)
  #PREVIOUS_RELEASE: XX

  # Metadata release (for use when the release was before rel25)
  #METADATA_REL: XX

  # Where is the BQ table dataset:
  SCRATCH_DATASET: bq_scratch_dataset_name

  # Dataset to publish to:
  PUBLICATION_DATASET: bq_scratch_dataset_name_without_versioned

  ## !Update these parameter per user of the workflow! ##

  # What is the file path to the text file in the bucket:
  WORKING_BUCKET_DIR: bq_staging_bucket_name # DO NOT HAVE A LEADING /

  # Where do we stash the manifest in the bucket after we get it:
  BUCKET_MANIFEST_TSV: bucket_path_for_raw_bq_table.tsv # DO NOT HAVE A LEADING /

  # Where do we stash the pull_list in the bucket after we get it:
  BUCKET_PULL_LIST: bucket_path_for_pull_list.txt # DO NOT HAVE A LEADING /

  # Where do we dump the schema git repository?
  SCHEMA_REPO_LOCAL: git_repo_for_schema

  # What directory holds the processed schema, tag, and desc lists?
  PROX_DESC_PREFIX: scratch_dir_for_schema

  ## !These parameters are rarely updated for the workflow! ##

  # Which Genomic Build?
  BUILD: hgXX

  # What is the data type?
  DATA_TYPE: copy_number_segment_allelic or copy_number_segment

  # What project are we in:
  WORKING_PROJECT: working_project_id

  # What bucket is going to get the text file heading to BQ?
  WORKING_BUCKET: bq_staging_bucket_name

  # Project to publish to:
  PUBLICATION_PROJECT: publication_project_id

  # Maximum number of files to download. Useful for testing before doing full download.
  #MAX_FILES: 10

  # GDC downloader fills out a directory tree. Here is the root:
  LOCAL_FILES_DIR: directory_root_for_files

  # Where is the table that allows us to build manifest:
  FILEDATA_TABLE: project_name.dataset_name.filedata_active_table

  # Where is the table that maps cases to primary site:
  CASE_TABLE: project_name.dataset_name.caseData

  # Where is the table that we will use to replace indexD calls:
  INDEXD_BQ_TABLE: project_name.manifest_dataset_name.gdc_d{}_hg38

  # Where do we locally stash the pull_list after we get it, or where is it if already provided:
  LOCAL_PULL_LIST: pull_list_file_full_path_and_file.tsv

  # Download creates a directory tree, which we need to traverse on upload. Put it here:
  FILE_TRAVERSAL_LIST: traversal_list_file_full_path_and_file.tsv

  # We glue all the files into one big one, here:
  ONE_BIG_TSV: raw_data_file_relative_path_and_file.tsv

  # Where do we stash the manifest after we get it:
  MANIFEST_FILE: manifest_file_relative_path_and_file.tsv

  # Where is the table that maps aliquot ID to all the other stuff (e.g. case barcode):
  ALIQUOT_TABLE: project_name.dataset_name.aliquot2caseIDmap

  # Where is the repo?
  SCHEMA_REPO_URL: https://github.com/schema_repo_url

  # What directory holds the schema data files in the repo?
  RAW_SCHEMA_DIR: schema_dir

  # What repo branch to use?
  SCHEMA_REPO_BRANCH: schema_branch

  # Name of schema file:
  SCHEMA_FILE_NAME: schema_file_name.json

  # Name of versioned schema file:
  VER_SCHEMA_FILE_NAME: versioned.schema_file_name.json

  # Saved schema dict (all fields):
  HOLD_SCHEMA_DICT: saved_schema_dict_name.json

  # Saved schema list (typed tuples from schema analysis only)
  HOLD_SCHEMA_LIST: saved_skel_schema_name.json

  # Number of rows to skip while sampling big TSV to generate schema:
  SCHEMA_SAMPLE_SKIPS: 1

# You can go to the GDC Data Portal Repository, build a filter, then click on the "Advanced Search"
# button to see the key-value pairs you want:

bq_filters:
   - program_name: PROGRAM
   - access: open
   - data_format: TXT
   - data_type: Data Type
   - platform: platform
   - data_category: Data Category

# Which tables are we updating schema on?
# Default is both

update_schema_tables:
  - versioned
  - current

# The table schema from the repo is generic, and needs to be individualized for each table
# with the following tags. "~-" means use above parameter, "~lc-" does the same with conversion to lower case, "~lcbqs-" does this as well, plus changes "." characters to "_"

schema_tags:

    - ---tag-program--- : ~-programs
    - ---tag-release-month-year--- : Month YYY
    - ---tag-release-url-anchor--- : data-release-XX0
    - ---tag-ref-genome-0--- : ~lc-builds
    - ---tag-release--- : Rel XX
    - ---tag-source-0--- : ~lcbqs-programs

steps:
  # Empty out the LOCAL_FILES_DIR first:
  - clear_target_directory
  # Get a manifest from our filters:
  - build_manifest_from_filters
  # Build the pull list from the manifest:
  - build_pull_list

  # Run the downloader on the manifest (caution: long!) into LOCAL_FILES_DIR
  - download_from_gdc
  # Build a file list from traversing LOCAL_FILES_DIR:
  - build_file_list

  # Build the ONE_BIG_TSV file from the little files:
  - concat_all_files

  # Check the position columns for scientific notation
  - check_position_data_type
  # If scientific notation is found in the position columns, fix to int
  # ONLY needs to be run if scientific notation is found
  - fix_position_data

  # Upload ONE_BIG_TSV to WORKING_BUCKET/WORKING_BUCKET_DIR/BUCKET_TSV:
  - upload_to_bucket

  # Get the table schema/description/tags pulled from git:
  - pull_table_info_from_git
  # Extract the table schema/description/tags from that file:
  - process_git_schemas
  # Customize generic schema:
  - replace_schema_tags
  # Analyze the schema from the ONE_BIG_TSV file:
  - analyze_the_schema

  # Load BQ table TARGET_DATASET.TARGET_TABLE:
  - create_bq_from_tsv
  # Fold in fields from ALIQUOT_TABLE to SCRATCH_DATASET.FINAL_SCRATCH_TABLE
  - add_aliquot_fields
  # Merge rows that have pooled aliquots
  - merge_same_aliq_samples

  # Create curernt table draft
  - create_current_table

  # Install final table schema descriptions:
  - update_field_descriptions
  # Add a table description:
  - update_table_description

  # Remove old current table
  # This doesn't need to be run if it is the first table of it's kind
  - compare_remove_old_current
  # publish the tables:
  - publish
  # Update previous versioned table to the 'archived' tag
  - update_status_tag

  # Delete working tables:
  - dump_working_tables
